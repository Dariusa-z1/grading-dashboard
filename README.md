# ğŸ“Š Grading Analysis Dashboard 

A **local Streamlit dashboard** for researchers or AI grading system builders to analyze and visualize automated grading results compared with TA (human) scores.  
It is designed to help evaluate model performance, detect grading issues, and support improvements to AI-based assessment systems.

---

## What it does
- ğŸ“‚ **Load CSV** of grading results and validate required columns.
- ğŸ“ **Compute metrics**: MAE, RMSE, MAPE, bias, correlations.
- ğŸš¨ **Auto-flag risky items** (low confidence, large errors).
- ğŸ“ˆ **Interactive plots**: scatter, histograms, box plots, Blandâ€“Altman.
- ğŸ” **Filter** by question, confidence, and error threshold.
- ğŸ’¾ **Export** processed/flagged data and generate a Markdown report.

---

## ğŸ“„ CSV schema
**Required columns**
- `student_id` (str)
- `question_id` (str)
- `ta_score` (float)
- `llm_score` (float)
- `max_points` (float/int)

**Optional columns (auto-added if missing)**
- `confidence` (float in [0,1], default 1.0)
- `flags` (bool, default False)
---

## ğŸ— Project Structure  
```
grading-dashboard/
â”œâ”€ main.py
â”œâ”€ requirements.txt
â”œâ”€ assets/
â”‚  â””â”€ style.css
â””â”€ src/
   â”œâ”€ __init__.py
   â”œâ”€ config.py
   â”œâ”€ utils/
   â”‚  â”œâ”€ __init__.py
   â”‚  â””â”€ session.py
   â”œâ”€ services/
   â”‚  â”œâ”€ __init__.py
   â”‚  â”œâ”€ data_service.py
   â”‚  â””â”€ analytics_service.py
   â””â”€ ui/
      â”œâ”€ __init__.py
      â”œâ”€ components.py
      â”œâ”€ layout.py
      â””â”€ tabs/
         â”œâ”€ __init__.py
         â”œâ”€ overview.py
         â”œâ”€ statistics.py
         â”œâ”€ deep_dive.py
         â”œâ”€ review_queue.py
         â””â”€ export_tab.py
```

---



## ğŸš¨ Auto-flag criteria
An item is flagged if **any** is true:
- `confidence < 0.6`
- `percent_error > 25` (|LLMâˆ’TA| / max_points * 100)
- `abs_error > 30%` of `max_points`

---

## ğŸ“Š Key metrics (per dataset / filtered view)
- **MAE**, **RMSE**, **MAPE**, **max error**
- **Pearson r** and **Spearman r**
- **Mean bias** and **std of error**
- Flag counts and percentages
- Question-level summary (MAE, confidence, flags, count)

---

## âš™ï¸ Install
```bash
python -m venv .venv && source .venv/bin/activate  # (Windows: .venv\Scripts\activate)
pip install -r requirements.txt
```

## â–¶ï¸ Run
```bash
streamlit run main.py
```
Then upload your CSV from the sidebar.

## ğŸ—‚ Tabs overview
- **ğŸ“ˆ Overview** â€” KPIs, agreement scatter, error histogram, question box plot.
- **ğŸ“Š Statistical Analysis** â€” Metrics, Blandâ€“Altman, question table.
- **ğŸ” Deep Dive** â€” Per-question drill-down, student rankings.
- **âš ï¸ Review Queue** â€” Flagged items table + CSV download.
- **ğŸ“‘ Export** â€” Download processed data or summary report.

## ğŸ“¤ Export outputs
- **review_queue_YYYYMMDD_HHMMSS.csv** (flagged items)
- **grading_analysis_YYYYMMDD_HHMMSS.csv** (full/filtered data)
- **report_YYYYMMDD_HHMMSS.md** (summary report)

## ğŸ’¡ Notes
- ğŸ§ª Generate a sample dataset from the welcome screen.
- ğŸš© Large errors or low confidence suggest items to review.

